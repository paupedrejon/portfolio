"""
Explanation Agent - Transforma informaci√≥n en explicaciones claras y resumidas
Genera apuntes estructurados del contenido procesado
"""

# Aplicar parche de proxies antes de importar ChatOpenAI
import sys
import os
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)
try:
    import openai_proxy_patch  # noqa: F401
    openai_proxy_patch.patch_langchain_openai()
except:
    pass

from typing import List, Dict, Optional
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from memory.memory_manager import MemoryManager
import tiktoken
import sys

# Importar model_manager
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

try:
    from model_manager import ModelManager
except ImportError:
    ModelManager = None
    print("‚ö†Ô∏è Warning: model_manager no disponible, usando OpenAI directamente")

class ExplanationAgent:
    """
    Agente especializado en generar explicaciones claras y resumidas
    """
    
    def __init__(self, memory: MemoryManager, api_key: Optional[str] = None, mode: str = "auto"):
        """
        Inicializa el agente de explicaciones
        
        Args:
            memory: Gestor de memoria del sistema
            api_key: API key de OpenAI (opcional)
            mode: Modo de selecci√≥n de modelo ("auto" = optimizar costes, "manual" = usar modelo especificado)
        """
        self.memory = memory
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.mode = mode
        self.llm = None  # Se inicializar√° cuando se necesite
        self.model_manager = None
        self.current_model_config = None
        
        # Inicializar model_manager si est√° disponible
        if ModelManager:
            try:
                self.model_manager = ModelManager(api_key=self.api_key, mode=mode)
                print("ü§ñ Explanation Agent inicializado con ModelManager (modo autom√°tico)")
            except Exception as e:
                print(f"‚ö†Ô∏è Warning: No se pudo inicializar ModelManager: {e}")
                self.model_manager = None
        else:
            # Fallback a OpenAI directo
            if self.api_key:
                try:
                    self.llm = ChatOpenAI(
                        model="gpt-3.5-turbo",  # Usar modelo m√°s barato por defecto
                        temperature=0.7,
                        api_key=self.api_key,
                        max_tokens=None
                    )
                    print("ü§ñ Explanation Agent inicializado (sin ModelManager, usando gpt-3.5-turbo)")
                except Exception as e:
                    print(f"‚ö†Ô∏è Warning: No se pudo inicializar el LLM: {e}")
            else:
                print("‚ö†Ô∏è Explanation Agent inicializado sin API key (se requerir√° para usar)")
    
    def generate_explanations(self, max_concepts: int = 20) -> Dict[str, str]:
        """
        Genera explicaciones claras del contenido procesado
        
        Args:
            max_concepts: N√∫mero m√°ximo de conceptos a explicar
            
        Returns:
            Diccionario con explicaciones por concepto o texto completo
        """
        # Usar model_manager si est√° disponible (modo autom√°tico)
        if self.model_manager:
            try:
                # Seleccionar modelo autom√°ticamente (prioriza gratis > barato > caro)
                # Para generaci√≥n de explicaciones, necesitamos contexto amplio
                self.current_model_config, self.llm = self.model_manager.select_model(
                    task_type="generation",
                    min_quality="medium",
                    context_length=8000  # Necesitamos contexto amplio
                )
                print(f"‚úÖ Usando modelo: {self.current_model_config.name} (costo: ${self.current_model_config.cost_per_1k_input:.4f}/{self.current_model_config.cost_per_1k_output:.4f} por 1k tokens)")
            except Exception as e:
                error_msg = f"‚ö†Ô∏è Error al seleccionar modelo autom√°ticamente: {str(e)}"
                print(error_msg)
                # Fallback: intentar con OpenAI si hay API key
                if self.api_key:
                    try:
                        self.llm = ChatOpenAI(
                            model="gpt-3.5-turbo",  # Modelo m√°s barato
                            temperature=0.7,
                            api_key=self.api_key,
                            max_tokens=None
                        )
                        print("‚úÖ Fallback a gpt-3.5-turbo")
                    except Exception as e2:
                        return {
                            "error": f"Error al inicializar el modelo: {str(e2)}",
                            "status": "error"
                        }
                else:
                    return {
                        "error": "Se requiere configurar una API key de OpenAI o tener Ollama instalado.",
                        "status": "error"
                    }
        else:
            # Fallback: usar OpenAI directamente
            if not self.api_key:
                return {
                    "error": "Se requiere configurar una API key de OpenAI para generar explicaciones.",
                    "status": "error"
                }
            
            # Inicializar LLM si no est√° inicializado
            if not self.llm:
                try:
                    # Usar gpt-3.5-turbo (m√°s barato) en lugar de gpt-4-turbo
                    self.llm = ChatOpenAI(
                        model="gpt-3.5-turbo",
                        temperature=0.7,
                        api_key=self.api_key,
                        max_tokens=None
                    )
                except Exception as e:
                    return {
                        "error": f"Error al inicializar el modelo: {str(e)}",
                        "status": "error"
                    }
        
        # Recuperar todo el contenido de la memoria
        all_content = self.memory.get_all_documents(limit=100)
        
        if not all_content:
            return {"error": "No hay contenido procesado. Sube documentos primero."}
        
        # Combinar contenido en chunks m√°s grandes
        combined_content = "\n\n---\n\n".join(all_content[:max_concepts * 2])
        
        # Generar explicaciones estructuradas
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", """Eres un profesor experto que explica conceptos de manera clara y sencilla.
            Tu tarea es transformar informaci√≥n compleja en explicaciones f√°ciles de entender.
            Mant√©n un tono educativo y amigable.
            Organiza la informaci√≥n de manera estructurada y l√≥gica."""),
            ("user", """Transforma el siguiente contenido educativo en explicaciones claras y estructuradas.

CONTENIDO:
{content}

Genera un documento de apuntes que incluya:
1. **Resumen Ejecutivo**: Un resumen breve de los conceptos principales
2. **Conceptos Clave**: Explicaci√≥n clara de cada concepto importante
3. **Ejemplos Pr√°cticos**: Ejemplos que ayuden a entender los conceptos
4. **Relaciones entre Conceptos**: C√≥mo se relacionan los diferentes temas
5. **Puntos Importantes a Recordar**: Lista de los puntos m√°s relevantes

Formato el resultado en Markdown con encabezados, listas y secciones bien organizadas.""")
        ])
        
        try:
            chain = prompt_template | self.llm
            explanation = chain.invoke({"content": combined_content})
            
            return {
                "explanations": explanation.content,
                "status": "success",
                "concepts_covered": len(all_content)
            }
        except Exception as e:
            return {
                "error": f"Error al generar explicaciones: {str(e)}",
                "status": "error"
            }
    
    def generate_notes(self, topics: Optional[List[str]] = None, model: Optional[str] = None, user_level: Optional[int] = None, conversation_history: Optional[List[dict]] = None, topic: Optional[str] = None) -> str:
        """
        Genera resumen completo de la conversaci√≥n en formato Markdown
        
        Args:
            topics: Lista de temas espec√≠ficos a cubrir (opcional)
            model: Modelo preferido (opcional, si no se especifica usa modo autom√°tico)
            user_level: Nivel del usuario en el tema (1-10, opcional)
            conversation_history: Historial de conversaci√≥n para generar resumen actualizado (opcional)
            
        Returns:
            Resumen en formato Markdown
        """
        # Usar model_manager si est√° disponible (modo autom√°tico)
        if self.model_manager:
            try:
                # Seleccionar modelo autom√°ticamente (prioriza gratis > barato > caro)
                # Para generaci√≥n de apuntes, necesitamos contexto amplio y buena calidad
                self.current_model_config, self.llm = self.model_manager.select_model(
                    task_type="generation",
                    min_quality="medium",
                    preferred_model=model if model else None,
                    context_length=8000  # Necesitamos contexto amplio
                )
                print(f"‚úÖ Usando modelo: {self.current_model_config.name} (costo: ${self.current_model_config.cost_per_1k_input:.4f}/{self.current_model_config.cost_per_1k_output:.4f} por 1k tokens)")
            except Exception as e:
                error_msg = f"‚ö†Ô∏è Error al seleccionar modelo autom√°ticamente: {str(e)}"
                print(error_msg)
                # Fallback: intentar con OpenAI si hay API key
                if self.api_key:
                    try:
                        self.llm = ChatOpenAI(
                            model="gpt-3.5-turbo",  # Modelo m√°s barato
                            temperature=0.7,
                            api_key=self.api_key,
                            max_tokens=None
                        )
                        print("‚úÖ Fallback a gpt-3.5-turbo")
                    except Exception as e2:
                        return f"# Error\n\n‚ö†Ô∏è Error al inicializar el modelo: {str(e2)}"
                else:
                    return "# Error\n\n‚ö†Ô∏è Se requiere configurar una API key de OpenAI o tener Ollama instalado. Por favor, configura tu API key o instala Ollama."
        else:
            # Fallback: usar OpenAI directamente
            if not self.api_key:
                return "# Error\n\n‚ö†Ô∏è Se requiere configurar una API key de OpenAI para generar apuntes. Por favor, configura tu API key."
            
            # Usar modelo especificado o el m√°s barato disponible
            model_to_use = model if model else "gpt-3.5-turbo"  # Por defecto usar el m√°s barato
            
            try:
                self.llm = ChatOpenAI(
                    model=model_to_use,
                    temperature=0.7,
                    api_key=self.api_key,
                    max_tokens=None
                )
            except Exception as e:
                # Si el modelo especificado falla, intentar con gpt-3.5-turbo como fallback
                try:
                    print(f"‚ö†Ô∏è Modelo {model_to_use} no disponible, usando gpt-3.5-turbo como fallback")
                    self.llm = ChatOpenAI(
                        model="gpt-3.5-turbo",
                        temperature=0.7,
                        api_key=self.api_key,
                        max_tokens=None
                    )
                except Exception as e2:
                    return f"# Error\n\n‚ö†Ô∏è Error al inicializar el modelo: {str(e2)}"
        
        # Definir el prompt template que se usar√° en ambos casos
        # Usar raw string (r"""...""") para evitar problemas con secuencias de escape
        prompt_template = r"""Eres un Arquitecto de Conocimiento experto en crear 'Hojas de Estudio de Alto Rendimiento'.

Tu objetivo NO es resumir, sino destilar la informaci√≥n para que sea memorizable al instante.

CONTENIDO FUENTE:
{content}

---

### üß† REGLAS DE ORO (STYLE GUIDE):

1. **CERO RELLENO:** Prohibido usar frases introductorias como "En este documento...", "A continuaci√≥n...", "Es importante notar que...". Ve directo al dato.

2. **FORMATO AT√ìMICO:** Usa Bullet points (‚Ä¢) para todo. P√°rrafos de m√°ximo 2 l√≠neas.

3. **VISUAL:** Usa **Negritas** para conceptos clave y `c√≥digo` para t√©rminos t√©cnicos.

4. **NO MERMAID:** Absolutamente prohibido usar bloques ```mermaid. Si necesitas un diagrama, usa SOLO el formato JSON especificado abajo.

### üìä ADAPTACI√ìN AL NIVEL (CR√çTICO):

El nivel del estudiante est√° indicado en {level_note}. **AD√ÅPTATE ESTRICTAMENTE AL NIVEL**:

**NIVEL 0-1 (Principiante Absoluto):**
- Solo vocabulario esencial: saludos, n√∫meros 1-10, colores b√°sicos
- Frases de supervivencia: "Hola", "Adi√≥s", "Gracias", "¬øC√≥mo est√°s?"
- Pronunciaci√≥n b√°sica explicada con letras
- Sin gram√°tica compleja, solo estructuras simples
- Ejemplos muy simples y comunes

**NIVEL 2-3 (Principiante):**
- Vocabulario cotidiano: d√≠as de la semana, meses, familia, comida b√°sica
- Frases simples: "Me llamo...", "Tengo hambre", "¬øCu√°nto cuesta?"
- Gram√°tica b√°sica: presente simple, art√≠culos b√°sicos
- Pronunciaci√≥n con gu√≠as fon√©ticas simples
- Ejemplos pr√°cticos de uso diario

**NIVEL 4-5 (Intermedio B√°sico):**
- Vocabulario tem√°tico: trabajo, viajes, hobbies, emociones
- Tiempos verbales: presente, pasado simple, futuro cercano
- Estructuras complejas b√°sicas: condicionales simples, comparativos
- Frases √∫tiles para situaciones comunes
- Ejemplos contextualizados

**NIVEL 6-7 (Intermedio):**
- Vocabulario avanzado tem√°tico: negocios, tecnolog√≠a, cultura
- Tiempos verbales complejos: subjuntivo, condicional, perfecto
- Expresiones idiom√°ticas comunes
- Gram√°tica avanzada: voz pasiva, construcciones impersonales
- Diferencias regionales b√°sicas
- Ejemplos de uso formal e informal

**NIVEL 8-9 (Avanzado):**
- Vocabulario sofisticado: t√©rminos acad√©micos, literarios, t√©cnicos
- Tiempos verbales avanzados: pluscuamperfecto, subjuntivo complejo
- Expresiones idiom√°ticas raras y cultas
- Gram√°tica compleja: per√≠frasis, construcciones estil√≠sticas
- Diferencias regionales detalladas (dialectos, acentos)
- Matices y sutilezas del idioma
- Ejemplos de literatura o discursos formales

**NIVEL 10 (Experto):**
- Vocabulario arcaico, literario o extremadamente espec√≠fico
- Construcciones gramaticales raras o poco comunes
- Expresiones idiom√°ticas obsoletas o regionales muy espec√≠ficas
- Excepciones y casos especiales
- Variaciones dialectales y sociolectales
- Referencias culturales y hist√≥ricas
- Uso estil√≠stico avanzado y figuras ret√≥ricas
- Ejemplos de textos cl√°sicos o acad√©micos especializados

### üìê ESTRUCTURA DE SALIDA OBLIGATORIA:

# {topic_name}

## ‚ö° Conceptos Blitz (Lo esencial)
*Lista r√°pida de definiciones clave. Formato: **Concepto**: Definici√≥n ultra-corta.*

## üìö N√∫cleo del Conocimiento
*Organiza el contenido por subtemas. Usa tablas siempre que sea posible para comparar.*

*Si es IDIOMAS (AD√ÅPTATE AL NIVEL):*
- **Nivel 0-3**: Tablas simples: | Palabra | Traducci√≥n | Pronunciaci√≥n (letras) |
- **Nivel 4-6**: Tablas ampliadas: | Vocabulario | Traducci√≥n | Contexto/Ejemplo | Notas |
- **Nivel 7-9**: Tablas avanzadas: | T√©rmino | Traducci√≥n Literal | Uso | Contexto Formal/Informal | Variaciones Regionales |
- **Nivel 10**: Tablas expertas: | T√©rmino | Etimo | Uso Arcaico/Moderno | Variantes Dialectales | Referencias Culturales |

*Si es PROGRAMACI√ìN (AD√ÅPTATE AL NIVEL):*
- **Nivel 0-3**: C√≥digo simple con comentarios l√≠nea por l√≠nea, sin conceptos complejos
- **Nivel 4-6**: Bloques de c√≥digo con comentarios explicativos y conceptos intermedios
- **Nivel 7-9**: C√≥digo avanzado con patrones, mejores pr√°cticas, optimizaciones
- **Nivel 10**: C√≥digo experto con arquitecturas complejas, patrones avanzados, casos edge

*Si es TEOR√çA:* Usa listas anidadas, adaptando la complejidad al nivel.

## ‚ö†Ô∏è Errores Comunes / Trampas
*Lista de cosas donde los estudiantes suelen fallar o confundirse.*

## üíé Ejemplo Pr√°ctico
*Un caso de uso real, frase completa o snippet de c√≥digo.*

---

### üé® INSTRUCCIONES PARA DIAGRAMAS (JSON ONLY):

Si el contenido se beneficia de una visualizaci√≥n (jerarqu√≠as, procesos, comparaciones VS), genera UN bloque de c√≥digo `diagram-json` al final de la secci√≥n correspondiente.

**Plantilla JSON Estricta:**

```diagram-json
{
  "title": "T√≠tulo del Diagrama",
  "nodes": [
    {"id": "A", "label": "Concepto Central", "color": "#6366f1"},
    {"id": "B", "label": "Subconcepto", "color": "#10b981", "description": "Explicaci√≥n breve"}
  ],
  "edges": [
    {"from": "A", "to": "B"}
  ]
}
```

**IMPORTANTE**: 
- SOLO genera diagramas para comparaciones directas de DOS elementos (ej: "A vs B")
- NO generes diagramas para vocabulario, frases, estructuras gramaticales, o listas de conceptos
- Si tienes dudas, NO generes diagrama. Usa listas o tablas en su lugar.

{level_note}"""

        # Preparar historial de conversaci√≥n si est√° disponible
        conversation_text = ""
        if conversation_history:
            conversation_text = "\n\n=== HISTORIAL DE CONVERSACI√ìN ===\n"
            for msg in conversation_history:
                role = msg.get("role", "unknown")
                content = msg.get("content", "")
                if role == "user":
                    conversation_text += f"\n[ESTUDIANTE]: {content}\n"
                elif role == "assistant":
                    conversation_text += f"\n[PROFESOR]: {content}\n"
            conversation_text += "\n=== FIN DEL HISTORIAL ===\n"
        
        # Usar topic si est√° disponible y topics no lo est√°
        final_topics = topics
        if not final_topics and topic:
            final_topics = [topic]
        
        # Determinar si hay historial de conversaci√≥n relevante al tema
        has_relevant_conversation = False
        if conversation_text and final_topics:
            # Verificar si el historial menciona el tema
            main_topic_lower = final_topics[0].lower() if isinstance(final_topics, list) else str(final_topics).lower()
            if main_topic_lower in conversation_text.lower():
                has_relevant_conversation = True
                print(f"üìù Historial de conversaci√≥n contiene informaci√≥n sobre '{final_topics[0]}'")
        
        if final_topics:
            # Si hay historial relevante, usarlo primero
            if has_relevant_conversation:
                combined_content = conversation_text
                print(f"üìù Usando historial de conversaci√≥n sobre '{final_topics[0]}'")
            else:
                # Si no hay historial relevante pero hay tema, generar desde cero
                # NO buscar en documentos gen√©ricos que pueden no ser relevantes
                if conversation_text:
                    # Si hay historial pero no es relevante, a√∫n as√≠ incluirlo pero priorizar el tema
                    print(f"üìù Historial de conversaci√≥n no es relevante para '{final_topics[0]}', generando desde cero con el tema")
                else:
                    print(f"üìù No hay historial de conversaci√≥n, generando apuntes educativos desde cero para '{final_topics[0]}'")
                
                main_topic = final_topics[0] if isinstance(final_topics, list) else str(final_topics)
                combined_content = f"TEMA: {main_topic}\n\nEste resumen se generar√° bas√°ndose en el conocimiento educativo sobre {main_topic}, adaptado al nivel del estudiante."
                
                # Si hay historial no relevante, a√±adirlo al final pero con menor prioridad
                if conversation_text:
                    combined_content += f"\n\n---\n\nHISTORIAL DE CONVERSACI√ìN (contexto adicional):\n{conversation_text}"
        else:
            # Obtener contenido pero limitar usando conteo real de tokens
            all_content = self.memory.get_all_documents(limit=100)  # Aumentar l√≠mite para obtener m√°s contenido
            
            # Si hay historial de conversaci√≥n pero no hay documentos, usar solo el historial
            if conversation_text and (not all_content or len(all_content) == 0):
                print("üìù Usando solo historial de conversaci√≥n (no hay documentos)")
                combined_content = conversation_text
            elif not all_content or len(all_content) == 0:
                # Si no hay historial ni documentos
                if not conversation_text:
                    print("‚ùå No hay documentos en la memoria ni historial de conversaci√≥n")
                    return "# Resumen\n\n‚ö†Ô∏è No hay contenido disponible. Por favor, sube documentos o inicia una conversaci√≥n primero."
                else:
                    combined_content = conversation_text
            else:
                # Filtrar documentos vac√≠os o muy cortos
                all_content = [doc for doc in all_content if doc and doc.strip() and len(doc.strip()) > 10]
                
                if not all_content or len(all_content) == 0:
                    # Si no hay documentos v√°lidos pero hay historial, usar solo historial
                    if conversation_text:
                        print("üìù Usando solo historial de conversaci√≥n (documentos vac√≠os)")
                        combined_content = conversation_text
                    else:
                        print("‚ùå Todos los documentos est√°n vac√≠os o son muy cortos")
                        return "# Resumen\n\n‚ö†Ô∏è Los documentos procesados no contienen suficiente contenido. Por favor, sube documentos con m√°s texto."
                else:
                    print(f"üìÑ Encontrados {len(all_content)} documentos con contenido v√°lido")
                    print(f"üìÑ Primer documento (primeros 200 chars): {all_content[0][:200]}...")
                    
                    # Calcular tokens usando tiktoken para asegurar que no excedemos el l√≠mite
                    try:
                        encoding = tiktoken.encoding_for_model("gpt-4")
                    except:
                        encoding = tiktoken.get_encoding("cl100k_base")
                    
                    # Aumentar l√≠mite de tokens ya que estamos usando gpt-4-turbo o gpt-4o que tienen m√°s contexto
                    MAX_CONTENT_TOKENS = 8000  # Aumentado para modelos con m√°s contexto
                    combined_content = ""
                    combined_tokens = 0
                    
                    # Calcular tokens del prompt base (sin contenido)
                    prompt_base_tokens = len(encoding.encode(prompt_template.replace("{content}", "")))
                    print(f"üìä Tokens del prompt base: {prompt_base_tokens}")
                    print(f"üìä L√≠mite de tokens para contenido: {MAX_CONTENT_TOKENS}")
                    
                    # A√±adir historial primero si est√° disponible (tiene prioridad)
                    if conversation_text:
                        hist_tokens = len(encoding.encode(conversation_text))
                        if hist_tokens + prompt_base_tokens <= MAX_CONTENT_TOKENS:
                            combined_content = conversation_text
                            combined_tokens = hist_tokens
                            print(f"üìù Historial a√±adido ({hist_tokens} tokens)")
                    
                    for i, doc in enumerate(all_content):
                        doc_text = f"\n\n---\n\n{doc}" if combined_content else doc
                        doc_tokens = len(encoding.encode(doc_text))
                        
                        # Verificar si a√±adir este documento exceder√≠a el l√≠mite
                        if combined_tokens + doc_tokens + prompt_base_tokens > MAX_CONTENT_TOKENS:
                            print(f"üìä L√≠mite alcanzado despu√©s de {i} documentos ({combined_tokens} tokens)")
                            # A√±adir nota de que hay m√°s contenido
                            combined_content += f"\n\n---\n\n[Nota: Hay m√°s contenido disponible. Se han incluido {i} documentos de {len(all_content)} disponibles. Para ver todo, puedes hacer preguntas espec√≠ficas sobre temas concretos.]"
                            break
                        combined_content += doc_text
                        combined_tokens += doc_tokens
                        print(f"üìÑ Documento {i+1} a√±adido ({doc_tokens} tokens, total: {combined_tokens})")
                    
                    print(f"üìä Contenido final: {combined_tokens} tokens, {len(combined_content)} caracteres")
        
        # Si no hay contenido ni temas, retornar error
        if not combined_content or not combined_content.strip():
            return "# Resumen\n\n‚ö†Ô∏è No hay contenido disponible. Por favor, sube documentos, inicia una conversaci√≥n, o especifica un tema."
        
        # Verificar que el contenido no est√© vac√≠o despu√©s de limpiar
        # Si tiene "TEMA:" al inicio, es contenido generado desde cero, as√≠ que permitirlo aunque sea corto
        if len(combined_content.strip()) < 50 and not combined_content.strip().startswith("TEMA:"):
            return "# Resumen\n\n‚ö†Ô∏è El contenido disponible es demasiado corto o est√° vac√≠o. Por favor, sube documentos con m√°s contenido o inicia una conversaci√≥n."
        
        # A√±adir validaci√≥n: mostrar una muestra del contenido para debugging
        print(f"‚úÖ Generando apuntes con {len(combined_content)} caracteres de contenido")
        print(f"üìÑ Primeros 500 caracteres: {combined_content[:500]}...")
        print(f"üìÑ √öltimos 200 caracteres: {combined_content[-200:]}...")
        
        # Validar que el contenido tiene informaci√≥n real (no solo espacios o caracteres especiales)
        # Si tiene "TEMA:" al inicio, es contenido generado desde cero, as√≠ que permitirlo
        if not combined_content.strip().startswith("TEMA:"):
            content_words = combined_content.split()
            if len(content_words) < 10:
                print(f"‚ùå Contenido tiene muy pocas palabras: {len(content_words)}")
                return "# Resumen\n\n‚ö†Ô∏è El contenido disponible tiene muy pocas palabras. Por favor, sube documentos con m√°s texto."
        
        # Validar palabras solo si no es contenido generado desde cero
        if not combined_content.strip().startswith("TEMA:"):
            content_words = combined_content.split()
            print(f"‚úÖ Contenido v√°lido: {len(content_words)} palabras")
        else:
            print(f"‚úÖ Generando contenido educativo desde cero para el tema especificado")
        
        # Preparar nota de nivel si est√° disponible
        level_note = ""
        if user_level is not None:
            if user_level <= 3:
                level_note = "\n\n**NIVEL DEL ESTUDIANTE**: Principiante (nivel {}/10). Adapta el contenido para que sea claro y accesible, usando lenguaje simple y explicaciones detalladas.".format(user_level)
            elif user_level <= 6:
                level_note = "\n\n**NIVEL DEL ESTUDIANTE**: Intermedio (nivel {}/10). Puedes usar terminolog√≠a t√©cnica pero siempre con explicaciones claras.".format(user_level)
            else:
                level_note = "\n\n**NIVEL DEL ESTUDIANTE**: Avanzado (nivel {}/10). Puedes usar terminolog√≠a t√©cnica avanzada y profundizar en los conceptos.".format(user_level)
        
        # Preparar nombre del tema
        topic_name = ""
        if final_topics and len(final_topics) > 0:
            topic_name = final_topics[0] if isinstance(final_topics, list) else str(final_topics)
        elif topic:
            topic_name = topic
        elif combined_content.strip().startswith("TEMA:"):
            # Extraer el tema del contenido generado
            import re
            topic_match = re.search(r'TEMA:\s*(.+)', combined_content)
            if topic_match:
                topic_name = topic_match.group(1).split('\n')[0].strip()
        
        if not topic_name:
            topic_name = "Estudio"
        
        # Usar replace directo en lugar de format para evitar problemas con llaves en el contenido
        # Esto es m√°s seguro cuando el contenido puede contener llaves tambi√©n
        prompt = prompt_template.replace("{content}", combined_content).replace("{level_note}", level_note).replace("{topic_name}", topic_name)

        try:
            print("üîÑ Invocando LLM para generar apuntes...")
            response = self.llm.invoke(prompt)
            notes_content = response.content
            print(f"‚úÖ Respuesta recibida: {len(notes_content)} caracteres")
            print(f"üìÑ Primeros 300 caracteres: {notes_content[:300]}")
            
            # POST-PROCESAMIENTO: Eliminar cualquier bloque Mermaid que el modelo pueda haber generado
            import re
            # Detectar y eliminar bloques de c√≥digo Mermaid (multil√≠nea)
            # Patr√≥n mejorado que captura bloques completos con cualquier contenido entre los backticks
            mermaid_patterns = [
                r'```\s*mermaid\s*\n.*?```',
                r'```\s*flowchart\s*\n.*?```',
                r'```\s*graph\s*\n.*?```',
                r'```\s*gantt\s*\n.*?```',
                r'```\s*sequenceDiagram\s*\n.*?```',
                r'```\s*classDiagram\s*\n.*?```',
                r'```\s*mindmap\s*\n.*?```',
            ]
            
            for pattern in mermaid_patterns:
                notes_content = re.sub(pattern, '', notes_content, flags=re.DOTALL | re.IGNORECASE | re.MULTILINE)
            
            # Tambi√©n eliminar bloques que comiencen directamente con comandos Mermaid (sin backticks iniciales)
            mermaid_code_patterns = [
                r'graph\s+(TB|TD|LR|RL|BT).*?```',
                r'flowchart\s+(TB|TD|LR|RL|BT).*?```',
                r'gantt\s+.*?```',
            ]
            
            for pattern in mermaid_code_patterns:
                notes_content = re.sub(pattern, '', notes_content, flags=re.DOTALL | re.IGNORECASE | re.MULTILINE)
            
            # Limpiar l√≠neas vac√≠as m√∫ltiples que puedan quedar despu√©s de eliminar bloques
            notes_content = re.sub(r'\n{3,}', '\n\n', notes_content)
            
            # Validar que hay bloques diagram-json en la respuesta
            diagram_json_count = notes_content.count('```diagram-json') + notes_content.count('``` diagram-json')
            if diagram_json_count == 0:
                print("‚ö†Ô∏è Advertencia: No se encontraron bloques diagram-json en la respuesta")
                print(f"üìÑ Primeros 500 caracteres de la respuesta: {notes_content[:500]}")
            else:
                print(f"‚úÖ Post-procesamiento: Bloques Mermaid eliminados, {diagram_json_count} diagrama(s) JSON encontrado(s)")
            
            # Validar que la respuesta no est√© vac√≠a
            if not notes_content or not notes_content.strip():
                return "# Error\n\n‚ö†Ô∏è La respuesta del modelo est√° vac√≠a. Por favor, intenta de nuevo."
            
            # Validar que la respuesta contenga contenido v√°lido
            if len(notes_content.strip()) < 50:
                return f"# Error\n\n‚ö†Ô∏è La respuesta del modelo es demasiado corta. Respuesta recibida: {notes_content[:200]}"
            
            return notes_content
        except KeyError as e:
            error_str = str(e)
            print(f"‚ùå KeyError al generar apuntes: {error_str}")
            import traceback
            traceback.print_exc()
            return f"# Error\n\n‚ö†Ô∏è Error al procesar la respuesta: {error_str}. Por favor, intenta de nuevo o verifica que hay contenido disponible."
        except Exception as e:
            error_str = str(e)
            print(f"‚ùå Error al generar apuntes: {error_str}")
            import traceback
            traceback.print_exc()
            # Si el error es por l√≠mite de tokens, sugerir usar menos contenido
            if "context_length" in error_str.lower() or "tokens" in error_str.lower():
                return f"# Error\n\nEl contenido es demasiado extenso. Por favor, intenta generar apuntes sobre temas espec√≠ficos o divide el documento en partes m√°s peque√±as.\n\nError: {error_str}"
            return f"# Error\n\nNo se pudieron generar los apuntes: {error_str}"
    
    def explain_concept(self, concept: str) -> str:
        """
        Explica un concepto espec√≠fico
        
        Args:
            concept: Concepto a explicar
            
        Returns:
            Explicaci√≥n del concepto
        """
        # Verificar API key
        if not self.api_key:
            return f"# Concepto: {concept}\n\n‚ö†Ô∏è Se requiere configurar una API key de OpenAI para explicar conceptos."
        
        # Inicializar LLM si no est√° inicializado
        if not self.llm:
            try:
                # Usar gpt-4-turbo que tiene 128k tokens de contexto (m√°s reciente y estable)
                # Si no est√° disponible, usar gpt-4o que tambi√©n tiene contexto amplio
                try:
                    self.llm = ChatOpenAI(
                        model="gpt-4-turbo",
                        temperature=0.7,
                        api_key=self.api_key,
                        max_tokens=None
                    )
                except:
                    # Fallback a gpt-4o si gpt-4-turbo no est√° disponible
                    self.llm = ChatOpenAI(
                        model="gpt-4o",
                        temperature=0.7,
                        api_key=self.api_key,
                        max_tokens=None
                    )
            except Exception as e:
                return f"# Error\n\n‚ö†Ô∏è Error al inicializar el modelo: {str(e)}"
        
        relevant_content = self.memory.retrieve_relevant_content(concept, n_results=5)
        
        if not relevant_content:
            return f"# Concepto: {concept}\n\nNo se encontr√≥ informaci√≥n sobre '{concept}' en los documentos procesados. Por favor, aseg√∫rate de haber subido documentos que contengan este concepto."
        
        prompt = f"""Explica el concepto '{concept}' de manera clara y completa bas√°ndote en el siguiente contenido:

{chr(10).join(relevant_content)}

Proporciona:
1. Definici√≥n clara
2. Explicaci√≥n detallada
3. Ejemplos pr√°cticos
4. Relaci√≥n con otros conceptos
5. Puntos importantes a recordar

Formato en Markdown."""

        try:
            response = self.llm.invoke(prompt)
            return response.content
        except Exception as e:
            return f"Error al explicar el concepto: {str(e)}"
