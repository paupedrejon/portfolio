"""
Explanation Agent - Transforma informaci√≥n en explicaciones claras y resumidas
Genera apuntes estructurados del contenido procesado
"""

# Aplicar parche de proxies antes de importar ChatOpenAI
import sys
import os
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)
try:
    import openai_proxy_patch  # noqa: F401
    openai_proxy_patch.patch_langchain_openai()
except:
    pass

from typing import List, Dict, Optional
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from memory.memory_manager import MemoryManager
import tiktoken
import sys
import requests
import re
import requests
import re

# Importar model_manager
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

try:
    from model_manager import ModelManager
except ImportError:
    ModelManager = None
    print("‚ö†Ô∏è Warning: model_manager no disponible, usando OpenAI directamente")

class ExplanationAgent:
    """
    Agente especializado en generar explicaciones claras y resumidas
    """
    
    def __init__(self, memory: MemoryManager, api_key: Optional[str] = None, mode: str = "auto"):
        """
        Inicializa el agente de explicaciones
        
        Args:
            memory: Gestor de memoria del sistema
            api_key: API key de OpenAI (opcional)
            mode: Modo de selecci√≥n de modelo ("auto" = optimizar costes, "manual" = usar modelo especificado)
        """
        self.memory = memory
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.mode = mode
        self.llm = None  # Se inicializar√° cuando se necesite
        self.model_manager = None
        self.current_model_config = None
        self.unsplash_api_key = os.getenv("UNSPLASH_API_KEY")  # Opcional: API key de Unsplash
        self.youtube_api_key = os.getenv("YOUTUBE_API_KEY")  # Opcional: API key de YouTube
        
        # Inicializar model_manager si est√° disponible
        if ModelManager:
            try:
                self.model_manager = ModelManager(api_key=self.api_key, mode=mode)
                print("ü§ñ Explanation Agent inicializado con ModelManager (modo autom√°tico)")
            except Exception as e:
                print(f"‚ö†Ô∏è Warning: No se pudo inicializar ModelManager: {e}")
                self.model_manager = None
        else:
            # Fallback a OpenAI directo
            if self.api_key:
                try:
                    self.llm = ChatOpenAI(
                        model="gpt-3.5-turbo",  # Usar modelo m√°s barato por defecto
                        temperature=0.7,
                        api_key=self.api_key,
                        max_tokens=None
                    )
                    print("ü§ñ Explanation Agent inicializado (sin ModelManager, usando gpt-3.5-turbo)")
                except Exception as e:
                    print(f"‚ö†Ô∏è Warning: No se pudo inicializar el LLM: {e}")
            else:
                print("‚ö†Ô∏è Explanation Agent inicializado sin API key (se requerir√° para usar)")
    
    def search_image(self, query: str) -> Optional[Dict[str, str]]:
        """
        Busca una imagen relevante usando Unsplash API o fallback a Pexels
        
        Args:
            query: T√©rmino de b√∫squeda para la imagen
            
        Returns:
            Diccionario con url, description, o None si no se encuentra
        """
        try:
            # Intentar con Unsplash primero (si hay API key)
            if self.unsplash_api_key:
                try:
                    search_url = "https://api.unsplash.com/search/photos"
                    params = {
                        "query": query,
                        "per_page": 1,
                        "orientation": "landscape",
                        "client_id": self.unsplash_api_key,
                    }
                    
                    response = requests.get(search_url, params=params, timeout=10)
                    response.raise_for_status()
                    data = response.json()
                    
                    if data.get("results") and len(data["results"]) > 0:
                        photo = data["results"][0]
                        image_url = photo["urls"]["regular"]
                        description = photo.get("description") or photo.get("alt_description") or query
                        return {
                            "url": image_url,
                            "description": description,
                            "source": "Unsplash",
                        }
                except Exception as e:
                    print(f"‚ö†Ô∏è Error usando Unsplash API: {e}, intentando Pexels")
            
            # Fallback a Pexels
            try:
                search_url = "https://api.pexels.com/v1/search"
                headers = {}
                if os.getenv("PEXELS_API_KEY"):
                    headers["Authorization"] = os.getenv("PEXELS_API_KEY")
                params = {
                    "query": query,
                    "per_page": 1,
                    "orientation": "landscape",
                }
                
                response = requests.get(search_url, headers=headers if headers else None, params=params, timeout=10)
                response.raise_for_status()
                data = response.json()
                
                if data.get("photos") and len(data["photos"]) > 0:
                    photo = data["photos"][0]
                    image_url = photo["src"]["large"]
                    description = photo.get("alt") or query
                    return {
                        "url": image_url,
                        "description": description,
                        "source": "Pexels",
                    }
            except Exception as e:
                print(f"‚ö†Ô∏è Error usando Pexels API: {e}")
            
            return None
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error buscando imagen: {e}")
            return None
    
    def search_youtube_video(self, query: str) -> Optional[Dict[str, str]]:
        """
        Busca un video relevante de YouTube usando la API o web scraping
        
        Args:
            query: T√©rmino de b√∫squeda para el video
            
        Returns:
            Diccionario con videoId, title, description, o None si no se encuentra
        """
        try:
            # Intentar con YouTube Data API v3 si hay API key
            if self.youtube_api_key:
                try:
                    search_url = "https://www.googleapis.com/youtube/v3/search"
                    params = {
                        "part": "snippet",
                        "q": query,
                        "type": "video",
                        "maxResults": 5,
                        "key": self.youtube_api_key,
                        "videoCategoryId": "27",  # Educaci√≥n
                        "order": "relevance",
                    }
                    
                    response = requests.get(search_url, params=params, timeout=10)
                    response.raise_for_status()
                    data = response.json()
                    
                    if data.get("items") and len(data["items"]) > 0:
                        # Buscar el video m√°s relevante
                        best_video = None
                        best_score = 0
                        
                        query_lower = query.lower()
                        educational_keywords = ["explicaci√≥n", "tutorial", "educativo", "aprender", "curso", "lecci√≥n", "explicar", "c√≥mo"]
                        has_educational = any(keyword in query_lower for keyword in educational_keywords)
                        
                        for item in data["items"]:
                            snippet = item.get("snippet", {})
                            title = snippet.get("title", "").lower()
                            description = snippet.get("description", "").lower()
                            
                            # Calcular score de relevancia
                            score = 0
                            query_words = query_lower.split()
                            
                            # Puntos por palabras coincidentes en el t√≠tulo
                            for word in query_words:
                                if word in title:
                                    score += 3
                                if word in description:
                                    score += 1
                            
                            # Bonus si tiene t√©rminos educativos (si no los tiene la query)
                            if not has_educational:
                                if any(keyword in title or keyword in description for keyword in educational_keywords):
                                    score += 2
                            
                            if score > best_score:
                                best_score = score
                                best_video = item
                        
                        if best_video:
                            video_id = best_video["id"]["videoId"]
                            snippet = best_video.get("snippet", {})
                            return {
                                "videoId": video_id,
                                "title": snippet.get("title", ""),
                                "description": snippet.get("description", ""),
                                "url": f"https://www.youtube.com/watch?v={video_id}",
                            }
                except Exception as e:
                    print(f"‚ö†Ô∏è Error usando YouTube API: {e}, intentando web scraping")
            
            # Fallback: web scraping b√°sico (sin API key)
            try:
                search_url = f"https://www.youtube.com/results?search_query={requests.utils.quote(query)}"
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                response = requests.get(search_url, headers=headers, timeout=10)
                response.raise_for_status()
                
                # Buscar video ID en el HTML (m√©todo b√°sico)
                import re
                video_id_pattern = r'"videoId":"([a-zA-Z0-9_-]{11})"'
                matches = re.findall(video_id_pattern, response.text)
                
                if matches:
                    video_id = matches[0]
                    return {
                        "videoId": video_id,
                        "title": query,
                        "description": "",
                        "url": f"https://www.youtube.com/watch?v={video_id}",
                    }
            except Exception as e:
                print(f"‚ö†Ô∏è Error usando web scraping de YouTube: {e}")
            
            return None
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error buscando video: {e}")
            return None
    
    def _process_image_blocks(self, content: str) -> str:
        """
        Procesa bloques de imagen en el contenido y los reemplaza con URLs reales
        
        Args:
            content: Contenido con bloques de imagen
            
        Returns:
            Contenido con bloques de imagen reemplazados por markdown de im√°genes
        """
        # Patr√≥n para detectar bloques de imagen
        image_block_pattern = r'```image\s*\n(.*?)```'
        
        def replace_image_block(match):
            block_content = match.group(1).strip()
            
            # Extraer query y description del bloque
            query = None
            description = None
            
            for line in block_content.split('\n'):
                if line.startswith('query:'):
                    query = line.replace('query:', '').strip()
                elif line.startswith('description:'):
                    description = line.replace('description:', '').strip()
            
            # Si no hay query, usar la description o el contenido completo
            if not query:
                query = description or block_content
            
            if not query:
                return ""
            
            # Buscar imagen
            image_data = self.search_image(query)
            
            if image_data:
                image_url = image_data["url"]
                image_desc = image_data.get("description", description or query)
                print(f"‚úÖ Imagen encontrada para '{query}': {image_url[:80]}...")
                
                # Reemplazar con markdown de imagen
                return f'\n\n![{image_desc}]({image_url})\n\n*Imagen: {image_desc}*\n\n'
            else:
                print(f"‚ö†Ô∏è No se encontr√≥ imagen para '{query}', dejando descripci√≥n")
                return f'\n\n*üí° Imagen sugerida: {description or query}*\n\n'
        
        # Reemplazar todos los bloques de imagen
        processed_content = re.sub(image_block_pattern, replace_image_block, content, flags=re.DOTALL | re.IGNORECASE)
        
        return processed_content
    
    def _process_video_blocks(self, content: str) -> str:
        """
        Procesa bloques de video de YouTube en el contenido y los reemplaza con informaci√≥n del video
        
        Args:
            content: Contenido con bloques de video
            
        Returns:
            Contenido con bloques de video reemplazados por informaci√≥n del video
        """
        # Patr√≥n para detectar bloques de video
        video_block_pattern = r'```youtube-video\s*\n(.*?)```'
        
        def replace_video_block(match):
            block_content = match.group(1).strip()
            
            # Extraer query y description del bloque
            query = None
            description = None
            
            for line in block_content.split('\n'):
                if line.startswith('query:'):
                    query = line.replace('query:', '').strip()
                elif line.startswith('description:'):
                    description = line.replace('description:', '').strip()
            
            # Si no hay query, usar la description o el contenido completo
            if not query:
                query = description or block_content
            
            if not query:
                return ""
            
            # Buscar video
            video_data = self.search_youtube_video(query)
            
            if video_data:
                video_id = video_data["videoId"]
                video_title = video_data.get("title", query)
                video_url = video_data.get("url", f"https://www.youtube.com/watch?v={video_id}")
                print(f"‚úÖ Video encontrado para '{query}': {video_id}")
                
                # Reemplazar con formato especial que el frontend puede procesar
                # El frontend detectar√° este formato y mostrar√° el visualizador
                return f'\n\n<youtube-video id="{video_id}" url="{video_url}" title="{video_title}" />\n\n'
            else:
                print(f"‚ö†Ô∏è No se encontr√≥ video para '{query}', dejando descripci√≥n")
                return f'\n\n*üé¨ Video sugerido: {description or query}*\n\n'
        
        # Reemplazar todos los bloques de video
        processed_content = re.sub(video_block_pattern, replace_video_block, content, flags=re.DOTALL | re.IGNORECASE)
        
        return processed_content
    
    def generate_explanations(self, max_concepts: int = 20) -> Dict[str, str]:
        """
        Genera explicaciones claras del contenido procesado
        
        Args:
            max_concepts: N√∫mero m√°ximo de conceptos a explicar
            
        Returns:
            Diccionario con explicaciones por concepto o texto completo
        """
        # Usar model_manager si est√° disponible (modo autom√°tico)
        if self.model_manager:
            try:
                # Seleccionar modelo autom√°ticamente (prioriza gratis > barato > caro)
                # Para generaci√≥n de explicaciones, necesitamos contexto amplio
                self.current_model_config, self.llm = self.model_manager.select_model(
                    task_type="generation",
                    min_quality="medium",
                    context_length=8000  # Necesitamos contexto amplio
                )
                print(f"‚úÖ Usando modelo: {self.current_model_config.name} (costo: ${self.current_model_config.cost_per_1k_input:.4f}/{self.current_model_config.cost_per_1k_output:.4f} por 1k tokens)")
            except Exception as e:
                error_msg = f"‚ö†Ô∏è Error al seleccionar modelo autom√°ticamente: {str(e)}"
                print(error_msg)
                # Fallback: intentar con OpenAI si hay API key
                if self.api_key:
                    try:
                        self.llm = ChatOpenAI(
                            model="gpt-3.5-turbo",  # Modelo m√°s barato
                            temperature=0.7,
                            api_key=self.api_key,
                            max_tokens=None
                        )
                        print("‚úÖ Fallback a gpt-3.5-turbo")
                    except Exception as e2:
                        return {
                            "error": f"Error al inicializar el modelo: {str(e2)}",
                            "status": "error"
                        }
                else:
                    return {
                        "error": "Se requiere configurar una API key de OpenAI o tener Ollama instalado.",
                        "status": "error"
                    }
        else:
            # Fallback: usar OpenAI directamente
            if not self.api_key:
                return {
                    "error": "Se requiere configurar una API key de OpenAI para generar explicaciones.",
                    "status": "error"
                }
            
            # Inicializar LLM si no est√° inicializado
            if not self.llm:
                try:
                    # Usar gpt-3.5-turbo (m√°s barato) en lugar de gpt-4-turbo
                    self.llm = ChatOpenAI(
                        model="gpt-3.5-turbo",
                        temperature=0.7,
                        api_key=self.api_key,
                        max_tokens=None
                    )
                except Exception as e:
                    return {
                        "error": f"Error al inicializar el modelo: {str(e)}",
                        "status": "error"
                    }
        
        # Recuperar todo el contenido de la memoria
        all_content = self.memory.get_all_documents(limit=100)
        
        if not all_content:
            return {"error": "No hay contenido procesado. Sube documentos primero."}
        
        # Combinar contenido en chunks m√°s grandes
        combined_content = "\n\n---\n\n".join(all_content[:max_concepts * 2])
        
        # Generar explicaciones estructuradas
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", """Eres un profesor experto que explica conceptos de manera clara y sencilla.
            Tu tarea es transformar informaci√≥n compleja en explicaciones f√°ciles de entender.
            Mant√©n un tono educativo y amigable.
            Organiza la informaci√≥n de manera estructurada y l√≥gica."""),
            ("user", """Transforma el siguiente contenido educativo en explicaciones claras y estructuradas.

CONTENIDO:
{content}

Genera un documento de apuntes que incluya:
1. **Resumen Ejecutivo**: Un resumen breve de los conceptos principales
2. **Conceptos Clave**: Explicaci√≥n clara de cada concepto importante
3. **Ejemplos Pr√°cticos**: Ejemplos que ayuden a entender los conceptos
4. **Relaciones entre Conceptos**: C√≥mo se relacionan los diferentes temas
5. **Puntos Importantes a Recordar**: Lista de los puntos m√°s relevantes

Formato el resultado en Markdown con encabezados, listas y secciones bien organizadas.""")
        ])
        
        try:
            chain = prompt_template | self.llm
            explanation = chain.invoke({"content": combined_content})
            
            return {
                "explanations": explanation.content,
                "status": "success",
                "concepts_covered": len(all_content)
            }
        except Exception as e:
            return {
                "error": f"Error al generar explicaciones: {str(e)}",
                "status": "error"
            }
    
    def generate_notes(self, topics: Optional[List[str]] = None, model: Optional[str] = None, user_level: Optional[int] = None, conversation_history: Optional[List[dict]] = None, topic: Optional[str] = None, chat_id: Optional[str] = None, user_id: Optional[str] = None) -> tuple[str, dict]:
        """
        Genera resumen completo de la conversaci√≥n en formato Markdown
        
        Args:
            topics: Lista de temas espec√≠ficos a cubrir (opcional)
            model: Modelo preferido (opcional, si no se especifica usa modo autom√°tico)
            user_level: Nivel del usuario en el tema (1-10, opcional)
            conversation_history: Historial de conversaci√≥n para generar resumen actualizado (opcional)
            
        Returns:
            Resumen en formato Markdown
        """
        # Usar model_manager si est√° disponible (modo autom√°tico)
        if self.model_manager:
            try:
                # Seleccionar modelo autom√°ticamente (prioriza gratis > barato > caro)
                # Para generaci√≥n de apuntes, necesitamos contexto amplio y buena calidad
                self.current_model_config, base_llm = self.model_manager.select_model(
                    task_type="generation",
                    min_quality="medium",
                    preferred_model=model if model else None,
                    context_length=8000  # Necesitamos contexto amplio
                )
                # Aumentar temperatura para m√°s variaci√≥n en los apuntes
                if hasattr(base_llm, 'temperature'):
                    base_llm.temperature = 0.9
                self.llm = base_llm
                print(f"‚úÖ Usando modelo: {self.current_model_config.name} (costo: ${self.current_model_config.cost_per_1k_input:.4f}/{self.current_model_config.cost_per_1k_output:.4f} por 1k tokens)")
            except Exception as e:
                error_msg = f"‚ö†Ô∏è Error al seleccionar modelo autom√°ticamente: {str(e)}"
                print(error_msg)
                # Fallback: intentar con OpenAI si hay API key
                if self.api_key:
                    try:
                        self.llm = ChatOpenAI(
                            model="gpt-3.5-turbo",  # Modelo m√°s barato
                            temperature=0.9,  # Aumentada para m√°s variaci√≥n
                            api_key=self.api_key,
                            max_tokens=None
                        )
                        print("‚úÖ Fallback a gpt-3.5-turbo")
                    except Exception as e2:
                        return f"# Error\n\n‚ö†Ô∏è Error al inicializar el modelo: {str(e2)}", {"inputTokens": 0, "outputTokens": 0, "model": None}
                else:
                    return "# Error\n\n‚ö†Ô∏è Se requiere configurar una API key de OpenAI o tener Ollama instalado. Por favor, configura tu API key o instala Ollama.", {"inputTokens": 0, "outputTokens": 0, "model": None}
        else:
            # Fallback: usar OpenAI directamente
            if not self.api_key:
                return "# Error\n\n‚ö†Ô∏è Se requiere configurar una API key de OpenAI para generar apuntes. Por favor, configura tu API key.", {"inputTokens": 0, "outputTokens": 0, "model": None}
            
            # Usar modelo especificado o el m√°s barato disponible
            model_to_use = model if model else "gpt-3.5-turbo"  # Por defecto usar el m√°s barato
            
            try:
                self.llm = ChatOpenAI(
                    model=model_to_use,
                    temperature=0.9,  # Aumentada para m√°s variaci√≥n en los apuntes
                    api_key=self.api_key,
                    max_tokens=None
                )
            except Exception as e:
                # Si el modelo especificado falla, intentar con gpt-3.5-turbo como fallback
                try:
                    print(f"‚ö†Ô∏è Modelo {model_to_use} no disponible, usando gpt-3.5-turbo como fallback")
                    self.llm = ChatOpenAI(
                        model="gpt-3.5-turbo",
                        temperature=0.9,  # Aumentada para m√°s variaci√≥n
                        api_key=self.api_key,
                        max_tokens=None
                    )
                except Exception as e2:
                    return f"# Error\n\n‚ö†Ô∏è Error al inicializar el modelo: {str(e2)}", {"inputTokens": 0, "outputTokens": 0, "model": None}
        
        # Definir el prompt template que se usar√° en ambos casos
        # Usar raw string (r"""...""") para evitar problemas con secuencias de escape
        prompt_template = r"""Eres un Arquitecto de Conocimiento experto en crear 'Hojas de Estudio de Alto Rendimiento'.

Tu objetivo NO es resumir, sino destilar la informaci√≥n para que sea memorizable al instante.

**üö® CR√çTICO - DETECCI√ìN DE TEMARIOS:**
Si el contenido fuente es un TEMARIO (lista de temas de examen/oposici√≥n), NO generes apuntes sobre "el temario", "qu√© temas entran" o "el contenido del examen". 
En su lugar, genera contenido educativo COMPLETO sobre CADA UNO de los temas listados. 
Crea apuntes detallados que cubran el contenido real de cada tema, como si fueras a ense√±ar ese tema desde cero.
Si hay m√∫ltiples temas, organiza el contenido por tema y cubre todos los temas listados con su contenido educativo completo.

{language_instruction}

### üé≤ VARIACI√ìN Y ORIGINALIDAD (IMPORTANTE):

**CR√çTICO**: Cada vez que generes apuntes, var√≠a el contenido significativamente:
- Usa diferentes ejemplos, palabras y frases
- Cambia el orden de los conceptos presentados
- Incluye contenido diferente pero igualmente relevante y √∫til
- NO repitas exactamente el mismo contenido de generaciones anteriores
- Si es un idioma, incluye vocabulario y frases diferentes cada vez
- Var√≠a los ejemplos pr√°cticos y casos de uso
- Presenta la informaci√≥n desde diferentes √°ngulos o enfoques

CONTENIDO FUENTE:
{content}

---

### üß† REGLAS DE ORO (STYLE GUIDE):

1. **CERO RELLENO:** Prohibido usar frases introductorias como "En este documento...", "A continuaci√≥n...", "Es importante notar que...". Ve directo al dato.

2. **FORMATO AT√ìMICO:** Usa Bullet points (‚Ä¢) para todo. P√°rrafos de m√°ximo 2 l√≠neas.

3. **VISUAL:** Usa **Negritas** para conceptos clave y `c√≥digo` para t√©rminos t√©cnicos.

4. **NO MERMAID:** Absolutamente prohibido usar bloques ```mermaid. Si necesitas un diagrama, usa SOLO el formato JSON especificado abajo.

### üìä ADAPTACI√ìN AL NIVEL (CR√çTICO - OBLIGATORIO):

**NIVEL ACTUAL DEL ESTUDIANTE: {user_level}/10**

{level_note}

**DEBES ADAPTAR ESTRICTAMENTE TODO EL CONTENIDO AL NIVEL {user_level}/10. NO uses contenido de otros niveles.**

**NIVEL 0-1 (Principiante Absoluto):**
- Solo vocabulario esencial: saludos, n√∫meros 1-10, colores b√°sicos
- Frases de supervivencia: "Hola", "Adi√≥s", "Gracias", "¬øC√≥mo est√°s?"
- Pronunciaci√≥n b√°sica explicada con letras
- Sin gram√°tica compleja, solo estructuras simples
- Ejemplos muy simples y comunes

**NIVEL 2-3 (Principiante):**
- Vocabulario cotidiano: d√≠as de la semana, meses, familia, comida b√°sica
- Frases simples: "Me llamo...", "Tengo hambre", "¬øCu√°nto cuesta?"
- Gram√°tica b√°sica: presente simple, art√≠culos b√°sicos
- Pronunciaci√≥n con gu√≠as fon√©ticas simples
- Ejemplos pr√°cticos de uso diario

**NIVEL 4-5 (Intermedio B√°sico):**
- Vocabulario tem√°tico: trabajo, viajes, hobbies, emociones
- Tiempos verbales: presente, pasado simple, futuro cercano
- Estructuras complejas b√°sicas: condicionales simples, comparativos
- Frases √∫tiles para situaciones comunes
- Ejemplos contextualizados

**NIVEL 6-7 (Intermedio):**
- Vocabulario avanzado tem√°tico: negocios, tecnolog√≠a, cultura
- Tiempos verbales complejos: subjuntivo, condicional, perfecto
- Expresiones idiom√°ticas comunes
- Gram√°tica avanzada: voz pasiva, construcciones impersonales
- Diferencias regionales b√°sicas
- Ejemplos de uso formal e informal

**NIVEL 8-9 (Avanzado):**
- Vocabulario sofisticado: t√©rminos acad√©micos, literarios, t√©cnicos
- Tiempos verbales avanzados: pluscuamperfecto, subjuntivo complejo
- Expresiones idiom√°ticas raras y cultas
- Gram√°tica compleja: per√≠frasis, construcciones estil√≠sticas
- Diferencias regionales detalladas (dialectos, acentos)
- Matices y sutilezas del idioma
- Ejemplos de literatura o discursos formales

**NIVEL 10 (Experto):**
- Vocabulario arcaico, literario o extremadamente espec√≠fico
- Construcciones gramaticales raras o poco comunes
- Expresiones idiom√°ticas obsoletas o regionales muy espec√≠ficas
- Excepciones y casos especiales
- Variaciones dialectales y sociolectales
- Referencias culturales y hist√≥ricas
- Uso estil√≠stico avanzado y figuras ret√≥ricas
- Ejemplos de textos cl√°sicos o acad√©micos especializados

### üìê ESTRUCTURA DE SALIDA OBLIGATORIA:

# {topic_name}

## ‚ö° Conceptos Blitz (Lo esencial)
*Lista r√°pida de definiciones clave. Formato: **Concepto**: Definici√≥n ultra-corta.*

## üìö N√∫cleo del Conocimiento
*Organiza el contenido por subtemas. Usa tablas siempre que sea posible para comparar.*

*Si es IDIOMAS (AD√ÅPTATE AL NIVEL):*
- **Nivel 0-3**: Tablas simples: | Palabra | Traducci√≥n | Pronunciaci√≥n (letras) |
- **Nivel 4-6**: Tablas ampliadas: | Vocabulario | Traducci√≥n | Contexto/Ejemplo | Notas |
- **Nivel 7-9**: Tablas avanzadas: | T√©rmino | Traducci√≥n Literal | Uso | Contexto Formal/Informal | Variaciones Regionales |
- **Nivel 10**: Tablas expertas: | T√©rmino | Etimo | Uso Arcaico/Moderno | Variantes Dialectales | Referencias Culturales |

*Si es PROGRAMACI√ìN (AD√ÅPTATE AL NIVEL):*
- **Nivel 0-3**: C√≥digo simple con comentarios l√≠nea por l√≠nea, sin conceptos complejos
- **Nivel 4-6**: Bloques de c√≥digo con comentarios explicativos y conceptos intermedios
- **Nivel 7-9**: C√≥digo avanzado con patrones, mejores pr√°cticas, optimizaciones
- **Nivel 10**: C√≥digo experto con arquitecturas complejas, patrones avanzados, casos edge

*Si es TEOR√çA:* Usa listas anidadas, adaptando la complejidad al nivel.

## ‚ö†Ô∏è Errores Comunes / Trampas
*Lista de cosas donde los estudiantes suelen fallar o confundirse.*

## üíé Ejemplo Pr√°ctico
*Un caso de uso real, frase completa o snippet de c√≥digo.*

---

### üé® INSTRUCCIONES PARA DIAGRAMAS (JSON ONLY) - REGLAS ESTRICTAS:

**üö´ PROHIBIDO GENERAR DIAGRAMAS PARA:**
- Vocabulario, palabras, frases o listas de t√©rminos
- Estructuras gramaticales o reglas de idioma
- Procesos, pasos o instrucciones
- Conceptos individuales o explicaciones de un solo tema
- Gu√≠as de uso, tutoriales o procedimientos
- Cualquier contenido que NO sea una comparaci√≥n directa de 2 elementos

**‚úÖ SOLO GENERA DIAGRAMAS SI:**
- Es una comparaci√≥n DIRECTA de EXACTAMENTE 2 elementos (ej: "Python vs JavaScript", "A vs B")
- El t√≠tulo contiene "vs", "versus", "contra" o "comparaci√≥n" seguido de 2 elementos
- Necesitas comparar caracter√≠sticas, ventajas/desventajas, o diferencias entre 2 cosas espec√≠ficas

**FORMATO OBLIGATORIO PARA COMPARACIONES:**

```diagram-json
{
  "title": "Elemento A vs Elemento B",
  "nodes": [
    {"id": "A", "label": "Elemento A", "color": "#6366f1"},
    {"id": "B", "label": "Elemento B", "color": "#10b981"}
  ],
  "edges": []
}
```

**REGLAS CR√çTICAS:**
- DEBE tener EXACTAMENTE 2 nodos (ni m√°s, ni menos)
- El t√≠tulo DEBE contener "vs", "versus", "contra" o "comparaci√≥n"
- NO uses edges (conexiones) en comparaciones
- Si NO es una comparaci√≥n de 2 elementos ‚Üí NO generes diagrama
- Si tienes CUALQUIER duda ‚Üí NO generes diagrama. Usa tablas o listas en su lugar.

### üñºÔ∏è INSTRUCCIONES PARA IM√ÅGENES:

**‚úÖ GENERA BLOQUES DE IMAGEN CUANDO:**
- Una imagen ayudar√≠a significativamente a explicar un concepto visual (ej: diagramas anat√≥micos, estructuras qu√≠micas, mapas, gr√°ficos)
- El concepto es dif√≠cil de entender sin una representaci√≥n visual
- Est√°s explicando algo que se beneficia de una ilustraci√≥n (ej: partes de una c√©lula, estructura de un √°tomo, mapa geogr√°fico)

**FORMATO OBLIGATORIO PARA IM√ÅGENES:**

```image
query: descripci√≥n de lo que debe mostrar la imagen
description: explicaci√≥n de c√≥mo la imagen ayuda a entender el concepto
```

**EJEMPLOS V√ÅLIDOS:**
```image
query: estructura de una c√©lula eucariota con org√°nulos etiquetados
description: Esta imagen muestra las partes principales de una c√©lula eucariota, ayudando a visualizar la organizaci√≥n celular.
```

```image
query: tabla peri√≥dica de elementos qu√≠micos
description: La tabla peri√≥dica muestra la organizaci√≥n de los elementos qu√≠micos seg√∫n su n√∫mero at√≥mico y propiedades.
```

**REGLAS:**
- Solo genera bloques de imagen cuando realmente a√±adan valor educativo
- La descripci√≥n debe explicar por qu√© la imagen es √∫til para entender el concepto
- Usa queries descriptivas y espec√≠ficas (en espa√±ol o ingl√©s)
- NO generes im√°genes para conceptos abstractos que no se beneficien de una visualizaci√≥n
- **IMPORTANTE**: Si el concepto se beneficiar√≠a de una imagen, SIEMPRE incluye un bloque de imagen

### üé¨ INSTRUCCIONES PARA VIDEOS:

**‚úÖ GENERA BLOQUES DE VIDEO CUANDO:**
- Un video explicativo ayudar√≠a significativamente a entender un concepto (ej: procesos, tutoriales, demostraciones)
- El concepto es mejor explicado con movimiento o secuencia (ej: c√≥mo funciona algo, pasos de un proceso)
- Est√°s explicando algo que se beneficia de una demostraci√≥n visual (ej: c√≥mo resolver un problema, c√≥mo usar una herramienta)
- El usuario expl√≠citamente pide un video o visualizaci√≥n

**FORMATO OBLIGATORIO PARA VIDEOS:**

```youtube-video
query: descripci√≥n de lo que debe explicar el video
description: explicaci√≥n de c√≥mo el video ayuda a entender el concepto
```

**EJEMPLOS V√ÅLIDOS:**
```youtube-video
query: c√≥mo funciona el sistema circulatorio explicaci√≥n educativa
description: Este video muestra c√≥mo funciona el sistema circulatorio con animaciones que ayudan a visualizar el flujo de sangre.
```

```youtube-video
query: tutorial c√≥mo resolver ecuaciones de segundo grado paso a paso
description: Este video explica paso a paso c√≥mo resolver ecuaciones de segundo grado con ejemplos pr√°cticos.
```

**REGLAS:**
- Solo genera bloques de video cuando realmente a√±adan valor educativo
- La descripci√≥n debe explicar por qu√© el video es √∫til para entender el concepto
- Usa queries descriptivas y espec√≠ficas (en espa√±ol o ingl√©s)
- **IMPORTANTE**: Si el concepto se beneficiar√≠a de un video, SIEMPRE incluye un bloque de video
- Prioriza videos educativos y tutoriales sobre entretenimiento

{level_note}"""

        # Preparar historial de conversaci√≥n si est√° disponible
        conversation_text = ""
        if conversation_history:
            conversation_text = "\n\n=== HISTORIAL DE CONVERSACI√ìN ===\n"
            for msg in conversation_history:
                role = msg.get("role", "unknown")
                content = msg.get("content", "")
                if role == "user":
                    conversation_text += f"\n[ESTUDIANTE]: {content}\n"
                elif role == "assistant":
                    conversation_text += f"\n[PROFESOR]: {content}\n"
            conversation_text += "\n=== FIN DEL HISTORIAL ===\n"
        
        # Usar topic si est√° disponible y topics no lo est√°
        final_topics = topics
        if not final_topics and topic:
            final_topics = [topic]
        
        # Detectar si el contenido es un temario (lista de temas) en lugar de contenido educativo
        def detect_and_extract_topics_from_syllabus(content: str) -> Optional[List[str]]:
            """Detecta si el contenido es un temario y extrae los temas listados"""
            if not content:
                return None
            
            content_lower = content.lower()
            
            # Palabras clave que indican que es un temario
            syllabus_keywords = [
                "temario", "temas del examen", "temas de la oposici√≥n", "temas a estudiar",
                "programa", "√≠ndice de temas", "lista de temas", "contenido del examen",
                "temas incluidos", "materias del examen"
            ]
            
            is_syllabus = any(keyword in content_lower for keyword in syllabus_keywords)
            
            if not is_syllabus:
                return None
            
            # Intentar extraer temas del temario
            import re
            topics = []
            
            # Patrones comunes para listas de temas:
            # 1. Tema 1: T√≠tulo del tema
            # 2. 1. T√≠tulo del tema
            # 3. - T√≠tulo del tema
            # 4. ‚Ä¢ T√≠tulo del tema
            
            patterns = [
                r'(?:tema|tema\s+\d+|^\d+\.)\s*[:\-]?\s*([^\n]+?)(?=\n(?:tema|tema\s+\d+|\d+\.|$))',
                r'^\d+\.\s*([^\n]+)',
                r'^[-‚Ä¢]\s*([^\n]+)',
                r'(?:tema|tema\s+\d+)\s+([^\n]+?)(?=\n(?:tema|tema\s+\d+|\d+\.|$))',
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, content, re.MULTILINE | re.IGNORECASE)
                if matches:
                    topics.extend([m.strip() for m in matches if len(m.strip()) > 5])  # Filtrar temas muy cortos
            
            # Si encontramos temas, devolverlos
            if topics:
                # Limpiar y deduplicar
                cleaned_topics = []
                seen = set()
                for topic in topics:
                    topic_clean = topic.strip()
                    # Eliminar prefijos comunes
                    topic_clean = re.sub(r'^(tema\s*\d+[:\-]?\s*)', '', topic_clean, flags=re.IGNORECASE)
                    topic_clean = re.sub(r'^\d+[:\-]?\s*', '', topic_clean)
                    if topic_clean and len(topic_clean) > 5 and topic_clean.lower() not in seen:
                        cleaned_topics.append(topic_clean)
                        seen.add(topic_clean.lower())
                
                if cleaned_topics:
                    print(f"üìã Temario detectado con {len(cleaned_topics)} temas extra√≠dos")
                    return cleaned_topics[:20]  # Limitar a 20 temas para no sobrecargar
        
            return None
        
        # Determinar si hay historial de conversaci√≥n relevante al tema
        has_relevant_conversation = False
        if conversation_text and final_topics:
            # Verificar si el historial menciona el tema
            main_topic_lower = final_topics[0].lower() if isinstance(final_topics, list) else str(final_topics).lower()
            if main_topic_lower in conversation_text.lower():
                has_relevant_conversation = True
                print(f"üìù Historial de conversaci√≥n contiene informaci√≥n sobre '{final_topics[0]}'")
        
        # Detectar si el contenido subido es un temario
        all_content = self.memory.get_all_documents(limit=50)
        syllabus_topics = None
        if all_content:
            combined_doc_content = "\n\n".join(all_content[:10])  # Revisar primeros documentos
            syllabus_topics = detect_and_extract_topics_from_syllabus(combined_doc_content)
            
            if syllabus_topics:
                print(f"üìã TEMARIO DETECTADO: Se encontraron {len(syllabus_topics)} temas en el documento")
                print(f"üìã Primeros temas: {', '.join(syllabus_topics[:5])}")
                # Si se detect√≥ un temario y el usuario pide apuntes/tests, usar los temas del temario
                if not final_topics or (len(final_topics) == 1 and final_topics[0].lower() in ["temario", "temas", "contenido"]):
                    final_topics = syllabus_topics
                    print(f"‚úÖ Usando temas del temario para generar contenido educativo")
        
        if final_topics:
            # Si hay historial relevante, usarlo primero
            if has_relevant_conversation:
                combined_content = conversation_text
                print(f"üìù Usando historial de conversaci√≥n sobre '{final_topics[0]}'")
            else:
                # Si no hay historial relevante pero hay tema, generar desde cero
                # NO buscar en documentos gen√©ricos que pueden no ser relevantes
                if conversation_text:
                    # Si hay historial pero no es relevante, a√∫n as√≠ incluirlo pero priorizar el tema
                    print(f"üìù Historial de conversaci√≥n no es relevante para '{final_topics[0]}', generando desde cero con el tema")
                else:
                    print(f"üìù No hay historial de conversaci√≥n, generando apuntes educativos desde cero para '{final_topics[0]}'")
                
                # Si hay m√∫ltiples temas (temario), generar contenido sobre todos ellos
                if isinstance(final_topics, list) and len(final_topics) > 1:
                    topics_list = "\n".join([f"- {t}" for t in final_topics[:15]])  # Limitar a 15 temas
                    combined_content = f"TEMARIO - TEMAS A PREPARAR:\n\n{topics_list}\n\nüö® INSTRUCCI√ìN CR√çTICA: El usuario quiere preparar estos temas de una oposici√≥n/examen. NO generes contenido sobre 'el temario' o 'qu√© temas entran'. En su lugar, genera contenido educativo COMPLETO sobre CADA UNO de estos temas. Crea apuntes detallados que cubran el contenido real de cada tema, como si fueras a ense√±ar ese tema desde cero. Organiza el contenido por tema y cubre todos los temas listados."
                else:
                    main_topic = final_topics[0] if isinstance(final_topics, list) else str(final_topics)
                    combined_content = f"TEMA: {main_topic}\n\nEste resumen se generar√° bas√°ndose en el conocimiento educativo sobre {main_topic}, adaptado al nivel del estudiante."
                
                # Si hay historial no relevante, a√±adirlo al final pero con menor prioridad
                if conversation_text:
                    combined_content += f"\n\n---\n\nHISTORIAL DE CONVERSACI√ìN (contexto adicional):\n{conversation_text}"
        else:
            # Obtener contenido pero limitar usando conteo real de tokens
            all_content = self.memory.get_all_documents(limit=100)  # Aumentar l√≠mite para obtener m√°s contenido
            
            # Si hay historial de conversaci√≥n pero no hay documentos, usar solo el historial
            if conversation_text and (not all_content or len(all_content) == 0):
                print("üìù Usando solo historial de conversaci√≥n (no hay documentos)")
                combined_content = conversation_text
            elif not all_content or len(all_content) == 0:
                # Si no hay historial ni documentos
                if not conversation_text:
                    print("‚ùå No hay documentos en la memoria ni historial de conversaci√≥n")
                    return "# Resumen\n\n‚ö†Ô∏è No hay contenido disponible. Por favor, sube documentos o inicia una conversaci√≥n primero."
                else:
                    combined_content = conversation_text
            else:
                # Filtrar documentos vac√≠os o muy cortos
                all_content = [doc for doc in all_content if doc and doc.strip() and len(doc.strip()) > 10]
                
                if not all_content or len(all_content) == 0:
                    # Si no hay documentos v√°lidos pero hay historial, usar solo historial
                    if conversation_text:
                        print("üìù Usando solo historial de conversaci√≥n (documentos vac√≠os)")
                        combined_content = conversation_text
                    else:
                        print("‚ùå Todos los documentos est√°n vac√≠os o son muy cortos")
                        return "# Resumen\n\n‚ö†Ô∏è Los documentos procesados no contienen suficiente contenido. Por favor, sube documentos con m√°s texto."
                else:
                    print(f"üìÑ Encontrados {len(all_content)} documentos con contenido v√°lido")
                    print(f"üìÑ Primer documento (primeros 200 chars): {all_content[0][:200]}...")
                    
                    # Calcular tokens usando tiktoken para asegurar que no excedemos el l√≠mite
                    try:
                        encoding = tiktoken.encoding_for_model("gpt-4")
                    except:
                        encoding = tiktoken.get_encoding("cl100k_base")
                    
                    # Aumentar l√≠mite de tokens ya que estamos usando gpt-4-turbo o gpt-4o que tienen m√°s contexto
                    MAX_CONTENT_TOKENS = 8000  # Aumentado para modelos con m√°s contexto
                    combined_content = ""
                    combined_tokens = 0
                    
                    # Calcular tokens del prompt base (sin contenido)
                    prompt_base_tokens = len(encoding.encode(prompt_template.replace("{content}", "")))
                    print(f"üìä Tokens del prompt base: {prompt_base_tokens}")
                    print(f"üìä L√≠mite de tokens para contenido: {MAX_CONTENT_TOKENS}")
                    
                    # A√±adir historial primero si est√° disponible (tiene prioridad)
                    if conversation_text:
                        hist_tokens = len(encoding.encode(conversation_text))
                        if hist_tokens + prompt_base_tokens <= MAX_CONTENT_TOKENS:
                            combined_content = conversation_text
                            combined_tokens = hist_tokens
                            print(f"üìù Historial a√±adido ({hist_tokens} tokens)")
                    
                    for i, doc in enumerate(all_content):
                        doc_text = f"\n\n---\n\n{doc}" if combined_content else doc
                        doc_tokens = len(encoding.encode(doc_text))
                        
                        # Verificar si a√±adir este documento exceder√≠a el l√≠mite
                        if combined_tokens + doc_tokens + prompt_base_tokens > MAX_CONTENT_TOKENS:
                            print(f"üìä L√≠mite alcanzado despu√©s de {i} documentos ({combined_tokens} tokens)")
                            # A√±adir nota de que hay m√°s contenido
                            combined_content += f"\n\n---\n\n[Nota: Hay m√°s contenido disponible. Se han incluido {i} documentos de {len(all_content)} disponibles. Para ver todo, puedes hacer preguntas espec√≠ficas sobre temas concretos.]"
                            break
                        combined_content += doc_text
                        combined_tokens += doc_tokens
                        print(f"üìÑ Documento {i+1} a√±adido ({doc_tokens} tokens, total: {combined_tokens})")
                    
                    print(f"üìä Contenido final: {combined_tokens} tokens, {len(combined_content)} caracteres")
        
        # Si no hay contenido ni temas, retornar error
        if not combined_content or not combined_content.strip():
            return "# Resumen\n\n‚ö†Ô∏è No hay contenido disponible. Por favor, sube documentos, inicia una conversaci√≥n, o especifica un tema."
        
        # Verificar que el contenido no est√© vac√≠o despu√©s de limpiar
        # Si tiene "TEMA:" al inicio, es contenido generado desde cero, as√≠ que permitirlo aunque sea corto
        if len(combined_content.strip()) < 50 and not combined_content.strip().startswith("TEMA:"):
            return "# Resumen\n\n‚ö†Ô∏è El contenido disponible es demasiado corto o est√° vac√≠o. Por favor, sube documentos con m√°s contenido o inicia una conversaci√≥n."
        
        # A√±adir validaci√≥n: mostrar una muestra del contenido para debugging
        print(f"‚úÖ Generando apuntes con {len(combined_content)} caracteres de contenido")
        print(f"üìÑ Primeros 500 caracteres: {combined_content[:500]}...")
        print(f"üìÑ √öltimos 200 caracteres: {combined_content[-200:]}...")
        
        # Validar que el contenido tiene informaci√≥n real (no solo espacios o caracteres especiales)
        # Si tiene "TEMA:" al inicio, es contenido generado desde cero, as√≠ que permitirlo
        if not combined_content.strip().startswith("TEMA:"):
            content_words = combined_content.split()
            if len(content_words) < 10:
                print(f"‚ùå Contenido tiene muy pocas palabras: {len(content_words)}")
                return "# Resumen\n\n‚ö†Ô∏è El contenido disponible tiene muy pocas palabras. Por favor, sube documentos con m√°s texto."
            print(f"‚úÖ Contenido v√°lido: {len(content_words)} palabras")
        else:
            print(f"‚úÖ Generando contenido educativo desde cero para el tema especificado")
        
        # Preparar nota de nivel si est√° disponible
        level_note = ""
        user_level_display = user_level if user_level is not None else 0
        if user_level is not None:
            if user_level <= 3:
                level_note = f"""
**NIVEL PRINCIPIANTE ({user_level}/10) - INSTRUCCIONES OBLIGATORIAS:**
- Solo vocabulario esencial: saludos, n√∫meros 1-10, colores b√°sicos
- Frases de supervivencia muy simples: "Hola", "Adi√≥s", "Gracias", "¬øC√≥mo est√°s?"
- Pronunciaci√≥n b√°sica explicada con letras
- Sin gram√°tica compleja, solo estructuras simples
- Ejemplos muy simples y comunes
- Lenguaje extremadamente simple y claro
- Explicaciones paso a paso muy detalladas"""
            elif user_level <= 6:
                level_note = f"""
**NIVEL INTERMEDIO ({user_level}/10) - INSTRUCCIONES OBLIGATORIAS:**
- Vocabulario tem√°tico: trabajo, viajes, hobbies, emociones
- Tiempos verbales: presente, pasado simple, futuro cercano
- Estructuras complejas b√°sicas: condicionales simples, comparativos
- Frases √∫tiles para situaciones comunes
- Ejemplos contextualizados
- Puedes usar terminolog√≠a t√©cnica pero siempre con explicaciones claras"""
            else:
                level_note = f"""
**NIVEL AVANZADO ({user_level}/10) - INSTRUCCIONES OBLIGATORIAS:**
- Vocabulario sofisticado: t√©rminos acad√©micos, literarios, t√©cnicos
- Tiempos verbales avanzados: pluscuamperfecto, subjuntivo complejo
- Expresiones idiom√°ticas raras y cultas
- Gram√°tica compleja: per√≠frasis, construcciones estil√≠sticas
- Diferencias regionales detalladas (dialectos, acentos)
- Matices y sutilezas del idioma
- Ejemplos de literatura o discursos formales
- Puedes usar terminolog√≠a t√©cnica avanzada y profundizar en los conceptos"""
        else:
            level_note = "\n\n**NIVEL NO ESPECIFICADO**: Usa nivel intermedio por defecto."
            user_level_display = 5
        
        # Preparar nombre del tema
        topic_name = ""
        if final_topics and len(final_topics) > 0:
            topic_name = final_topics[0] if isinstance(final_topics, list) else str(final_topics)
        elif topic:
            topic_name = topic
        elif combined_content.strip().startswith("TEMA:"):
            # Extraer el tema del contenido generado
            import re
            topic_match = re.search(r'TEMA:\s*(.+)', combined_content)
            if topic_match:
                topic_name = topic_match.group(1).split('\n')[0].strip()
        
        if not topic_name:
            topic_name = "Estudio"
        
        # Detectar si el tema es un idioma para generar en ese idioma
        is_language_topic = False
        language_name = ""
        if topic_name:
            topic_lower = topic_name.lower()
            language_map = {
                "ingl√©s": "ingl√©s", "english": "ingl√©s",
                "franc√©s": "franc√©s", "francais": "franc√©s", "french": "franc√©s",
                "alem√°n": "alem√°n", "deutsch": "alem√°n", "german": "alem√°n",
                "italiano": "italiano", "italian": "italiano",
                "portugu√©s": "portugu√©s", "portuguese": "portugu√©s",
                "chino": "chino", "chinese": "chino", "mandar√≠n": "chino",
                "japon√©s": "japon√©s", "japanese": "japon√©s", "japones": "japon√©s",
                "coreano": "coreano", "korean": "coreano",
                "ruso": "ruso", "russian": "ruso",
                "espa√±ol": "espa√±ol", "spanish": "espa√±ol",
            }
            for key, value in language_map.items():
                if key in topic_lower:
                    is_language_topic = True
                    language_name = value
                    break
        
        # Preparar instrucci√≥n de idioma
        language_instruction = ""
        if is_language_topic and language_name:
            language_instruction = f"""
### üåç IDIOMA DE GENERACI√ìN (CR√çTICO):

**IMPORTANTE**: El tema es "{topic_name}" ({language_name}). 

**ESTRUCTURA DE IDIOMAS:**
- Las **explicaciones, t√≠tulos y descripciones** deben estar en **ESPA√ëOL** (el idioma con el que te hablo)
- El **contenido del idioma objetivo** ({language_name}) debe estar en **{language_name.upper()}**
- Las **traducciones** deben mostrar: palabra en {language_name} ‚Üí traducci√≥n en espa√±ol

Ejemplo CORRECTO para Ingl√©s:
- T√≠tulo: "# Ingl√©s" (en espa√±ol)
- Concepto: "**Hello**: Hola." (palabra en ingl√©s, traducci√≥n en espa√±ol)
- Ejemplo: "Hello, how are you?": ¬øC√≥mo est√°s? (frase en ingl√©s, traducci√≥n en espa√±ol)

Ejemplo CORRECTO para Franc√©s:
- T√≠tulo: "# Franc√©s" (en espa√±ol)
- Concepto: "**Bonjour**: Buenos d√≠as." (palabra en franc√©s, traducci√≥n en espa√±ol)
- Ejemplo: "Bonjour, comment allez-vous?": Buenos d√≠as, ¬øc√≥mo est√° usted? (frase en franc√©s, traducci√≥n en espa√±ol)

**REGLA**: Explica en espa√±ol, pero muestra el contenido del idioma objetivo en {language_name}.
"""
        
        # Usar replace directo en lugar de format para evitar problemas con llaves en el contenido
        # Esto es m√°s seguro cuando el contenido puede contener llaves tambi√©n
        # Reemplazar tambi√©n {language_name} y {user_level} en el prompt
        # Asegurarse de que user_level_display est√© definido
        if 'user_level_display' not in locals():
            user_level_display = user_level if user_level is not None else 0
        
        final_prompt = prompt_template.replace("{content}", combined_content).replace("{level_note}", level_note).replace("{topic_name}", topic_name).replace("{language_instruction}", language_instruction).replace("{user_level}", str(user_level_display))
        if is_language_topic and language_name:
            final_prompt = final_prompt.replace("{language_name}", language_name)
        else:
            final_prompt = final_prompt.replace("{language_name}", "el idioma objetivo")
        
        prompt = final_prompt

        try:
            print("üîÑ Invocando LLM para generar apuntes...")
            response = self.llm.invoke(prompt)
            notes_content = response.content
            print(f"‚úÖ Respuesta recibida: {len(notes_content)} caracteres")
            print(f"üìÑ Primeros 300 caracteres: {notes_content[:300]}")
            
            # Capturar tokens de uso
            usage_info = {
                "inputTokens": 0,
                "outputTokens": 0,
                "model": None
            }
            
            # Determinar el modelo usado
            model_used = None
            if self.current_model_config:
                model_used = self.current_model_config.name
            elif model:
                model_used = model
            elif hasattr(self.llm, 'model_name'):
                model_used = self.llm.model_name
            elif hasattr(self.llm, 'model'):
                model_used = self.llm.model
            else:
                model_used = "gpt-3.5-turbo"  # Fallback
            
            usage_info["model"] = model_used
            
            # Intentar obtener tokens de la metadata de la respuesta
            if hasattr(response, 'response_metadata') and response.response_metadata:
                token_usage = response.response_metadata.get('token_usage', {})
                usage_info["inputTokens"] = token_usage.get('prompt_tokens', 0)
                usage_info["outputTokens"] = token_usage.get('completion_tokens', 0)
            else:
                # Estimar tokens si no est√°n disponibles (1 token ‚âà 4 caracteres)
                usage_info["inputTokens"] = len(prompt) // 4
                usage_info["outputTokens"] = len(notes_content) // 4
            
            # POST-PROCESAMIENTO: Eliminar cualquier bloque Mermaid y validar diagramas JSON
            import re
            import json as json_module
            
            # Detectar y eliminar bloques de c√≥digo Mermaid (multil√≠nea)
            # Patr√≥n mejorado que captura bloques completos con cualquier contenido entre los backticks
            mermaid_patterns = [
                r'```\s*mermaid\s*\n.*?```',
                r'```\s*flowchart\s*\n.*?```',
                r'```\s*graph\s*\n.*?```',
                r'```\s*gantt\s*\n.*?```',
                r'```\s*sequenceDiagram\s*\n.*?```',
                r'```\s*classDiagram\s*\n.*?```',
                r'```\s*mindmap\s*\n.*?```',
            ]
            
            for pattern in mermaid_patterns:
                notes_content = re.sub(pattern, '', notes_content, flags=re.DOTALL | re.IGNORECASE | re.MULTILINE)
            
            # Tambi√©n eliminar bloques que comiencen directamente con comandos Mermaid (sin backticks iniciales)
            mermaid_code_patterns = [
                r'graph\s+(TB|TD|LR|RL|BT).*?```',
                r'flowchart\s+(TB|TD|LR|RL|BT).*?```',
                r'gantt\s+.*?```',
            ]
            
            for pattern in mermaid_code_patterns:
                notes_content = re.sub(pattern, '', notes_content, flags=re.DOTALL | re.IGNORECASE | re.MULTILINE)
            
            # Limpiar l√≠neas vac√≠as m√∫ltiples que puedan quedar despu√©s de eliminar bloques
            notes_content = re.sub(r'\n{3,}', '\n\n', notes_content)
            
            # Validar y filtrar diagramas JSON: solo mantener comparaciones 1vs1 v√°lidas
            diagram_json_pattern = r'```\s*diagram-json\s*\n(.*?)```'
            diagram_matches = list(re.finditer(diagram_json_pattern, notes_content, re.DOTALL))
            
            diagrams_to_remove = []
            valid_diagrams_count = 0
            
            for match in diagram_matches:
                diagram_block = match.group(0)
                diagram_content = match.group(1).strip()
                
                try:
                    # Intentar parsear el JSON
                    diagram_data = json_module.loads(diagram_content)
                    
                    # Validar que sea una comparaci√≥n v√°lida
                    title = (diagram_data.get("title", "") or "").lower()
                    nodes = diagram_data.get("nodes", [])
                    
                    # Verificar que el t√≠tulo contenga palabras de comparaci√≥n
                    has_comparison_keyword = any(keyword in title for keyword in [" vs ", " versus", " contra ", " vs. ", "comparaci√≥n"])
                    
                    # Verificar que tenga exactamente 2 nodos
                    has_exactly_2_nodes = len(nodes) == 2
                    
                    # Si NO es una comparaci√≥n v√°lida, marcarlo para eliminaci√≥n
                    if not (has_comparison_keyword and has_exactly_2_nodes):
                        diagrams_to_remove.append(match)
                        print(f"üö´ Diagrama eliminado: '{diagram_data.get('title', 'Sin t√≠tulo')}' - No es comparaci√≥n v√°lida (nodos: {len(nodes)}, tiene 'vs': {has_comparison_keyword})")
                    else:
                        valid_diagrams_count += 1
                        print(f"‚úÖ Diagrama v√°lido: '{diagram_data.get('title', 'Sin t√≠tulo')}' - Comparaci√≥n 1vs1")
                        
                except (json_module.JSONDecodeError, AttributeError, TypeError) as e:
                    # Si no se puede parsear, eliminar el diagrama
                    diagrams_to_remove.append(match)
                    print(f"üö´ Diagrama eliminado: Error al parsear JSON - {e}")
            
            # Eliminar diagramas inv√°lidos (de atr√°s hacia adelante para no afectar √≠ndices)
            for match in reversed(diagrams_to_remove):
                notes_content = notes_content[:match.start()] + notes_content[match.end():]
            
            # Validar que hay bloques diagram-json v√°lidos en la respuesta
            total_diagrams = notes_content.count('```diagram-json') + notes_content.count('``` diagram-json')
            if valid_diagrams_count == 0 and total_diagrams > 0:
                print("‚ö†Ô∏è Todos los diagramas fueron eliminados por no ser comparaciones v√°lidas")
            elif valid_diagrams_count > 0:
                print(f"‚úÖ Post-procesamiento: {valid_diagrams_count} diagrama(s) v√°lido(s) de comparaci√≥n 1vs1")
            else:
                print("‚ÑπÔ∏è No se encontraron diagramas en la respuesta")
            
            # POST-PROCESAMIENTO: Procesar bloques de imagen y video
            notes_content = self._process_image_blocks(notes_content)
            notes_content = self._process_video_blocks(notes_content)
            
            # Validar que la respuesta no est√© vac√≠a
            if not notes_content or not notes_content.strip():
                return "# Error\n\n‚ö†Ô∏è La respuesta del modelo est√° vac√≠a. Por favor, intenta de nuevo.", {"inputTokens": 0, "outputTokens": 0, "model": model_used if 'model_used' in locals() else None}
            
            # Validar que la respuesta contenga contenido v√°lido
            if len(notes_content.strip()) < 50:
                return f"# Error\n\n‚ö†Ô∏è La respuesta del modelo es demasiado corta. Respuesta recibida: {notes_content[:200]}", {"inputTokens": 0, "outputTokens": 0, "model": model_used}
            
            return notes_content, usage_info
        except KeyError as e:
            error_str = str(e)
            print(f"‚ùå KeyError al generar apuntes: {error_str}")
            import traceback
            traceback.print_exc()
            model_used = self.current_model_config.name if self.current_model_config else (model or "gpt-3.5-turbo")
            return f"# Error\n\n‚ö†Ô∏è Error al procesar la respuesta: {error_str}. Por favor, intenta de nuevo o verifica que hay contenido disponible.", {"inputTokens": 0, "outputTokens": 0, "model": model_used}
        except Exception as e:
            error_str = str(e)
            print(f"‚ùå Error al generar apuntes: {error_str}")
            import traceback
            traceback.print_exc()
            model_used = self.current_model_config.name if self.current_model_config else (model or "gpt-3.5-turbo")
            # Si el error es por l√≠mite de tokens, sugerir usar menos contenido
            if "context_length" in error_str.lower() or "tokens" in error_str.lower():
                return f"# Error\n\nEl contenido es demasiado extenso. Por favor, intenta generar apuntes sobre temas espec√≠ficos o divide el documento en partes m√°s peque√±as.\n\nError: {error_str}", {"inputTokens": 0, "outputTokens": 0, "model": model_used}
            return f"# Error\n\nNo se pudieron generar los apuntes: {error_str}", {"inputTokens": 0, "outputTokens": 0, "model": model_used}
    
    def explain_concept(self, concept: str) -> str:
        """
        Explica un concepto espec√≠fico
        
        Args:
            concept: Concepto a explicar
            
        Returns:
            Explicaci√≥n del concepto
        """
        # Verificar API key
        if not self.api_key:
            return f"# Concepto: {concept}\n\n‚ö†Ô∏è Se requiere configurar una API key de OpenAI para explicar conceptos."
        
        # Inicializar LLM si no est√° inicializado
        if not self.llm:
            try:
                # Usar gpt-4-turbo que tiene 128k tokens de contexto (m√°s reciente y estable)
                # Si no est√° disponible, usar gpt-4o que tambi√©n tiene contexto amplio
                try:
                    self.llm = ChatOpenAI(
                        model="gpt-4-turbo",
                        temperature=0.7,
                        api_key=self.api_key,
                        max_tokens=None
                    )
                except:
                    # Fallback a gpt-4o si gpt-4-turbo no est√° disponible
                    self.llm = ChatOpenAI(
                        model="gpt-4o",
                        temperature=0.7,
                        api_key=self.api_key,
                        max_tokens=None
                    )
            except Exception as e:
                return f"# Error\n\n‚ö†Ô∏è Error al inicializar el modelo: {str(e)}"
        
        # Nota: explain_concept no tiene chat_id, pero deber√≠a tenerlo para mantener chats separados
        # Por ahora, sin chat_id no recuperar√° contenido (evita mezclar chats)
        relevant_content = self.memory.retrieve_relevant_content(concept, n_results=5, chat_id=None, user_id=None)
        
        if not relevant_content:
            return f"# Concepto: {concept}\n\nNo se encontr√≥ informaci√≥n sobre '{concept}' en los documentos procesados. Por favor, aseg√∫rate de haber subido documentos que contengan este concepto."
        
        prompt = f"""Explica el concepto '{concept}' de manera clara y completa bas√°ndote en el siguiente contenido:

{chr(10).join(relevant_content)}

Proporciona:
1. Definici√≥n clara
2. Explicaci√≥n detallada
3. Ejemplos pr√°cticos
4. Relaci√≥n con otros conceptos
5. Puntos importantes a recordar

Formato en Markdown."""

        try:
            response = self.llm.invoke(prompt)
            return response.content
        except Exception as e:
            return f"Error al explicar el concepto: {str(e)}"
